{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Engineering in Practice\n",
    "\n",
    "### Initialization\n",
    "- Importing DEEZER Brazil data set\n",
    "- Turning into a Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  item_id\n",
      "0    26371    24082\n",
      "1    26371   294519\n",
      "2    26371    46312\n",
      "3    26371   114070\n",
      "4    26371   216818\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1045554 entries, 0 to 1045553\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count    Dtype\n",
      "---  ------   --------------    -----\n",
      " 0   user_id  1045554 non-null  int64\n",
      " 1   item_id  1045554 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 16.0 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from anonypy import Preserver\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the .inter file \n",
    "file_path = r\"P:\\pCloud Offline\\PLUS\\2nd Sem\\Privacy Engineering\\Data\\DEEZER_BR.inter\"\n",
    "\n",
    "# Read the file\n",
    "deezer_data = pd.read_csv(file_path, delimiter=\",\")  \n",
    "deezer_data = deezer_data.rename(columns={'user_id:token': 'user_id','item_id:token': 'item_id'})\n",
    "\n",
    "# Display the first few rows\n",
    "print(deezer_data.head())\n",
    "\n",
    "# Check column names and data types\n",
    "print(deezer_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates so that what remains are interacted/not interacted with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 669686 entries, 0 to 1045550\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count   Dtype\n",
      "---  ------   --------------   -----\n",
      " 0   user_id  669686 non-null  int64\n",
      " 1   item_id  669686 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 15.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "deezer_data_unique = deezer_data.drop_duplicates(subset=['user_id', 'item_id'])\n",
    "print(deezer_data_unique.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\AppData\\Local\\Temp\\ipykernel_7848\\4082983075.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['interaction'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix: (10000, 144178)\n",
      "Number of interactions: 669686\n"
     ]
    }
   ],
   "source": [
    "df = deezer_data_unique\n",
    "df['interaction'] = 1\n",
    "\n",
    "# Encode user and item IDs to integer indices\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "user_indices = user_encoder.fit_transform(df['user_id'])\n",
    "item_indices = item_encoder.fit_transform(df['item_id'])\n",
    "\n",
    "# Build the binary interaction matrix\n",
    "interaction_matrix = csr_matrix(\n",
    "    (df['interaction'], (user_indices, item_indices))\n",
    ")\n",
    "\n",
    "print(\"Shape of matrix:\", interaction_matrix.shape)\n",
    "print(\"Number of interactions:\", interaction_matrix.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the sparse matrix into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       0       1       2       3       4       5       6       7       8       \\\n",
      "0          0       0       0       0       0       0       0       0       0   \n",
      "1          0       0       0       0       0       0       0       0       0   \n",
      "2          0       0       0       0       0       0       0       0       0   \n",
      "3          0       0       0       0       0       0       0       0       0   \n",
      "4          0       0       0       0       0       0       0       0       0   \n",
      "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "9995       0       0       0       0       0       0       0       0       0   \n",
      "9996       0       0       0       0       0       0       0       0       0   \n",
      "9997       0       0       0       0       0       0       0       0       0   \n",
      "9998       0       0       0       0       0       0       0       0       0   \n",
      "9999       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "      9       ...  144168  144169  144170  144171  144172  144173  144174  \\\n",
      "0          0  ...       0       0       0       0       0       0       0   \n",
      "1          0  ...       0       0       0       0       0       0       0   \n",
      "2          0  ...       0       0       0       0       0       0       0   \n",
      "3          0  ...       0       0       0       0       0       0       0   \n",
      "4          0  ...       0       0       0       0       0       0       0   \n",
      "...      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "9995       0  ...       0       0       0       0       0       0       0   \n",
      "9996       0  ...       0       0       0       0       0       0       0   \n",
      "9997       0  ...       0       0       0       0       0       0       0   \n",
      "9998       0  ...       0       0       0       0       0       0       0   \n",
      "9999       0  ...       0       0       0       0       0       0       0   \n",
      "\n",
      "      144175  144176  144177  \n",
      "0          0       0       0  \n",
      "1          0       0       0  \n",
      "2          0       0       0  \n",
      "3          0       0       0  \n",
      "4          0       0       0  \n",
      "...      ...     ...     ...  \n",
      "9995       0       0       0  \n",
      "9996       0       0       0  \n",
      "9997       0       0       0  \n",
      "9998       0       0       0  \n",
      "9999       0       0       0  \n",
      "\n",
      "[10000 rows x 144178 columns]>\n"
     ]
    }
   ],
   "source": [
    "dense_array = interaction_matrix.toarray()  # or use .todense()\n",
    "df_orig = pd.DataFrame(dense_array)\n",
    "print(df_orig.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a subset of the original data: filtering based on number of interactions (users/items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with few interactions: 9314\n",
      "Original shape: (10000, 144178)\n",
      "Cleaned shape: (686, 1065)\n",
      "Proportion of 1s: 0.10595819816860345\n"
     ]
    }
   ],
   "source": [
    "# sequential !! items first before users\n",
    "items_min_interaction = df_orig.sum(axis=0) < 100 # each item(column) should have at least ?? users\n",
    "print(f\"Number of columns with few interactions: {items_min_interaction.sum()}\")\n",
    "\n",
    "df_cleaned = df_orig.loc[:, ~items_min_interaction]\n",
    "\n",
    "users_min_interaction = df_cleaned.sum(axis=1) < 100 # each user(row) should have at least ?? items\n",
    "print(f\"Number of rows with few interactions: {users_min_interaction.sum()}\")\n",
    "\n",
    "df_cleaned = df_cleaned.loc[~users_min_interaction, :]\n",
    "original_shape = df_orig.shape\n",
    "cleaned_shape = df_cleaned.shape\n",
    "print(f\"Original shape: {original_shape}\")\n",
    "print(f\"Cleaned shape: {cleaned_shape}\")\n",
    "print(f\"Proportion of 1s: {((df_cleaned == 1).sum().sum())/(df_cleaned.shape[0]*df_cleaned.shape[1])}\") # sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mondrian implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     132    189    190    290    686    743    761    801    878    925  ...  \\\n",
      "0  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  ...   \n",
      "1  [0-1]    [0]  [0-1]  [0-1]    [0]    [0]    [0]  [0-1]  [0-1]  [0-1]  ...   \n",
      "2    [0]  [0-1]    [0]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]    [0]  ...   \n",
      "3    [0]  [0-1]  [0-1]  [0-1]  [0-1]    [0]    [0]  [0-1]  [0-1]  [0-1]  ...   \n",
      "4    [0]  [0-1]    [0]    [0]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  ...   \n",
      "\n",
      "  143265 143621 143879 143896 144003 144049 144131 144167 dummy_sensitive  \\\n",
      "0  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]               0   \n",
      "1    [0]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]    [0]               0   \n",
      "2    [0]  [0-1]  [0-1]  [0-1]    [0]  [0-1]  [0-1]  [0-1]               0   \n",
      "3    [0]  [0-1]    [0]  [0-1]    [0]  [0-1]  [0-1]  [0-1]               0   \n",
      "4    [0]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]    [0]               0   \n",
      "\n",
      "  count  \n",
      "0   137  \n",
      "1    19  \n",
      "2    21  \n",
      "3    26  \n",
      "4    17  \n",
      "\n",
      "[5 rows x 1067 columns]\n"
     ]
    }
   ],
   "source": [
    "KANON = 15 # change the k value\n",
    "\n",
    "for_anon = df_cleaned.copy()\n",
    "col_count = for_anon.shape[1]\n",
    "for_anon.columns = for_anon.columns.astype(str)\n",
    "\n",
    "for_anon['dummy_sensitive'] = 0 # dummy variable for the sensitive attribute (none for this analysis)\n",
    "\n",
    "sensitive_column = 'dummy_sensitive'\n",
    "feature_columns =  [str(col) for col in for_anon.columns if col != sensitive_column]\n",
    "\n",
    "p = Preserver(for_anon, feature_columns, sensitive_column)\n",
    "anonymized_data = p.anonymize_k_anonymity(k=KANON)  \n",
    "anonymized_df = pd.DataFrame(anonymized_data)\n",
    "print(anonymized_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same implementation as above but was modified to have a dummy variable that contains the row numbers in each group. This is needed for the flipping later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     132    189    190    290    686    743    761    801    878    925  ...  \\\n",
      "0  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  ...   \n",
      "1  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  ...   \n",
      "2  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  ...   \n",
      "3  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  ...   \n",
      "4  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  ...   \n",
      "\n",
      "  143265 143621 143879 143896 144003 144049 144131 144167 dummy_sensitive  \\\n",
      "0  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]              18   \n",
      "1  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]              20   \n",
      "2  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]              22   \n",
      "3  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]              24   \n",
      "4  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]  [0-1]              27   \n",
      "\n",
      "  count  \n",
      "0     1  \n",
      "1     1  \n",
      "2     1  \n",
      "3     1  \n",
      "4     1  \n",
      "\n",
      "[5 rows x 1067 columns]\n"
     ]
    }
   ],
   "source": [
    "for_anon = df_cleaned.copy()\n",
    "col_count = for_anon.shape[1]\n",
    "for_anon.columns = for_anon.columns.astype(str)\n",
    "\n",
    "for_anon['dummy_sensitive'] = range(len(for_anon))\n",
    "\n",
    "sensitive_column = 'dummy_sensitive'\n",
    "feature_columns =  [str(col) for col in for_anon.columns if col != sensitive_column]\n",
    "\n",
    "\n",
    "p = Preserver(for_anon, feature_columns, sensitive_column)\n",
    "anonymized_data = p.anonymize_k_anonymity(k=KANON)  \n",
    "anonymized_df_withrows = pd.DataFrame(anonymized_data)\n",
    "print(anonymized_df_withrows.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checker of the sizes of the groups formed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    count_value  frequency\n",
      "0            15          5\n",
      "1            21          4\n",
      "2            19          4\n",
      "3            22          4\n",
      "4            18          3\n",
      "5            16          2\n",
      "6            17          1\n",
      "7           137          1\n",
      "8            26          1\n",
      "9            25          1\n",
      "10           23          1\n",
      "11           29          1\n",
      "12           20          1\n"
     ]
    }
   ],
   "source": [
    "vc = anonymized_df['count'].value_counts()\n",
    "vc.name = 'frequency'\n",
    "freq_table = vc.reset_index()\n",
    "freq_table.columns = ['count_value', 'frequency']\n",
    "print(freq_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert group sizes and row number to lists in preparation for the flipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[137, 19, 21, 26, 17, 15, 19, 16, 25, 22, 22, 15, 15, 18, 21, 21, 21, 23, 15, 29, 15, 22, 18, 19, 20, 16, 18, 19, 22]\n",
      "[18, 20, 22, 24, 27, 39, 41, 53, 63, 65, 68, 71, 94, 103, 116, 118, 119, 126, 130, 138, 147, 148, 149, 150, 153, 164, 173, 180, 183, 187, 195, 203, 204, 206, 208, 212, 214, 215, 219, 226, 231, 234, 236, 250, 252, 256, 267, 269, 272, 275, 276, 278, 279, 280, 289, 295, 296, 307, 318, 320, 321, 323, 326, 332, 350, 358, 361, 366, 374, 377, 408, 409, 410, 412, 416, 419, 427, 434, 440, 441, 447, 449, 453, 454, 459, 461, 468, 479, 481, 484, 487, 488, 499, 501, 502, 504, 505, 521, 526, 532, 541, 544, 546, 550, 552, 554, 557, 565, 568, 571, 577, 583, 585, 586, 602, 603, 604, 608, 609, 613, 616, 621, 622, 627, 631, 636, 639, 640, 641, 643, 654, 657, 661, 669, 670, 672, 685, 110, 134, 161, 167, 225, 248, 310, 339, 389, 401, 404, 466, 498, 529, 537, 579, 645, 662, 676, 10, 78, 140, 141, 213, 217, 229, 232, 247, 262, 290, 316, 344, 364, 483, 495, 530, 549, 566, 619, 648, 35, 36, 48, 69, 84, 95, 186, 202, 283, 287, 346, 360, 365, 371, 406, 436, 451, 473, 486, 510, 555, 559, 567, 594, 615, 625, 19, 88, 102, 185, 224, 241, 270, 297, 300, 382, 411, 418, 462, 503, 563, 655, 656, 51, 86, 91, 96, 131, 162, 240, 304, 333, 433, 458, 611, 614, 626, 667, 15, 26, 28, 135, 174, 188, 189, 193, 227, 235, 260, 284, 325, 379, 403, 413, 428, 494, 524, 64, 90, 222, 246, 281, 291, 334, 347, 363, 367, 383, 386, 476, 533, 596, 610, 6, 45, 61, 97, 111, 263, 266, 294, 299, 302, 372, 392, 422, 475, 490, 515, 523, 531, 620, 633, 659, 671, 680, 683, 684, 12, 42, 59, 155, 184, 230, 301, 337, 345, 423, 432, 448, 482, 506, 518, 547, 560, 592, 597, 607, 635, 664, 4, 107, 114, 122, 129, 132, 144, 178, 182, 201, 335, 340, 400, 415, 430, 438, 442, 471, 512, 618, 632, 644, 11, 82, 104, 160, 172, 197, 220, 233, 322, 357, 359, 396, 402, 485, 536, 81, 83, 177, 293, 306, 378, 421, 431, 520, 527, 539, 545, 556, 575, 649, 2, 30, 52, 133, 136, 211, 330, 331, 355, 356, 362, 385, 395, 405, 470, 496, 599, 612, 38, 74, 99, 142, 152, 156, 170, 191, 223, 387, 435, 437, 445, 452, 489, 528, 578, 589, 617, 646, 682, 44, 46, 55, 57, 76, 108, 146, 158, 216, 319, 384, 439, 522, 558, 561, 562, 576, 591, 601, 624, 628, 105, 121, 123, 127, 166, 194, 200, 205, 277, 288, 368, 369, 381, 467, 509, 551, 553, 569, 573, 584, 677, 23, 32, 33, 34, 62, 72, 124, 125, 154, 274, 285, 354, 426, 446, 450, 457, 464, 511, 513, 623, 652, 665, 675, 9, 21, 58, 60, 85, 98, 120, 196, 268, 397, 414, 425, 497, 593, 630, 7, 13, 50, 77, 87, 100, 112, 115, 145, 176, 181, 199, 207, 238, 249, 312, 313, 417, 420, 460, 463, 465, 478, 500, 508, 534, 582, 605, 663, 29, 137, 169, 198, 253, 264, 271, 292, 336, 376, 407, 474, 581, 653, 660, 79, 92, 139, 163, 210, 254, 259, 273, 309, 338, 353, 393, 398, 443, 455, 456, 472, 493, 514, 570, 587, 658, 5, 40, 67, 113, 143, 165, 221, 243, 245, 324, 341, 352, 388, 469, 572, 637, 650, 673, 0, 47, 54, 75, 106, 192, 218, 237, 239, 251, 255, 349, 390, 424, 429, 477, 507, 535, 564, 3, 109, 128, 151, 171, 209, 228, 244, 261, 303, 305, 328, 351, 370, 375, 399, 516, 517, 647, 668, 37, 56, 70, 93, 101, 242, 257, 265, 282, 343, 394, 580, 588, 600, 629, 642, 1, 25, 49, 73, 80, 157, 159, 190, 311, 314, 327, 373, 380, 444, 492, 606, 651, 674, 8, 14, 16, 17, 43, 66, 117, 175, 308, 342, 348, 491, 519, 540, 574, 595, 598, 666, 681, 31, 89, 168, 179, 258, 286, 298, 315, 317, 329, 391, 480, 525, 538, 542, 543, 548, 590, 634, 638, 678, 679]\n"
     ]
    }
   ],
   "source": [
    "df_groups = df_cleaned.copy()\n",
    "df_groups['group_number'] = None\n",
    "\n",
    "# convert group sizes and row numbers to lists\n",
    "group_sizes = anonymized_df['count'].tolist()\n",
    "row_numbers = anonymized_df_withrows['dummy_sensitive'].tolist()\n",
    "\n",
    "print(group_sizes)\n",
    "print(row_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign groups iteratively\n",
    "df_groups.reset_index(drop=True, inplace=True)\n",
    "start_idx = 0\n",
    "for group_num, size in enumerate(group_sizes, start=1):\n",
    "    # Extract row numbers for current group\n",
    "    group_rows = row_numbers[start_idx:start_idx + size]\n",
    "    \n",
    "    # Assign group number to valid rows in A\n",
    "    for row in group_rows:\n",
    "        idx = row   # Convert to 0-based index\n",
    "        df_groups.at[idx, 'group_number'] = group_num\n",
    "    \n",
    "    start_idx += size  # Move to next group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymize each group by doing the flipping based on majority value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anonymize each group\n",
    "def anonymize_per_group(matrix_df):\n",
    "\n",
    "    # create working copy and remove group_number column\n",
    "    anonymized_mat = matrix_df.drop(columns=['group_number']).copy()\n",
    "    \n",
    "    # get group assignments from the list\n",
    "    groups = matrix_df.groupby('group_number').groups\n",
    "    \n",
    "    for group_id, row_indices in groups.items():\n",
    "        # extract cluster submatrix\n",
    "        cluster_mat = anonymized_mat.loc[row_indices]\n",
    "        \n",
    "        # process each column\n",
    "        for col in cluster_mat.columns:\n",
    "            col_sum = cluster_mat[col].sum()\n",
    "            if col_sum > 0:  # only process non-zero columns\n",
    "                # determine majority value \n",
    "                majority_val = 1 if col_sum >= len(row_indices)/2 else 0\n",
    "                # apply to all rows in cluster\n",
    "                anonymized_mat.loc[row_indices, col] = majority_val\n",
    "                \n",
    "    return anonymized_mat\n",
    "\n",
    "anonymized_A = anonymize_per_group(df_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting the dataset in preparation for recommender systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = []\n",
    "for user_id, row in anonymized_A.iterrows():\n",
    "    for item_id, value in row.items():\n",
    "        if value == 1:  # only include interactions\n",
    "            transactions.append({\n",
    "                'userID': user_id,\n",
    "                'itemID': item_id,\n",
    "                'rating': 1,  # dummy for recsys\n",
    "                'timestamp': 0  # dummy for recsys\n",
    "            })\n",
    "\n",
    "recsys_data = pd.DataFrame(transactions)\n",
    "recsys_data.head()\n",
    "recsys_data.to_csv(r'P:\\pCloud Offline\\PLUS\\2nd Sem\\Privacy Engineering\\Data\\Results\\mondrian_k15_75_50.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the Hamming distance loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08670526560724894\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_loss(original, anonymized) -> float:\n",
    "    return np.abs(original.values - anonymized.values).sum() / original.size\n",
    "\n",
    "print(compute_loss(df_cleaned,anonymized_A))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
